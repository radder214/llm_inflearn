{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구현 방식\n",
    "1. 문서의 내용을 읽는다.\n",
    "2. 문서를 쪼갠다.\n",
    "    - 토큰수 초과로 답변을 생성하지 못할 수 있고\n",
    "    - 문서가 길면(인풋이 길면) 답변 생성이 오래걸린다.\n",
    "3. (쪼갠 문서를) 임베딩 -> 벡터 DB에 저장\n",
    "4. 질문이 있을 때 벡터 DB에서 유사도 검색\n",
    "5. 유사도 검색으로 가져온 문서를 LLM에 질문과 같이 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 문서의 내용을 읽는다\n",
    "##### Langchain을 사용하지 않으므로 문서 parsing과 chunking을 내가 직접 해줘야한다.(1, 2번 과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "document = Document(\"./tax.docx\")\n",
    "full_text = \"\"\n",
    "for index, paragraph in enumerate(document.paragraphs):\n",
    "    print(f\"paragraph : {paragraph.text}\")\n",
    "    full_text += f\"{paragraph.text}\\n\"\n",
    "    # if index == 2: break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 문서를 쪼갠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "# encoding = encoder.encode(full_text) # text를 숫자 list로 바꾼다.\n",
    "# len(encoding) # 182997의 token을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded = encoder.decode(encoding) # 숫자 list를 다시 text로 바꾼다.\n",
    "# decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 cell 2개의 내용을 담은 함수를 만들어 chunk 해보자\n",
    "import tiktoken\n",
    "\n",
    "def split_text(full_text, chunk_size):\n",
    "    encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    total_encoding = encoder.encode(full_text) # text를 숫자 list로 바꾼다.\n",
    "    total_token_count = len(total_encoding)\n",
    "    text_list = []\n",
    "    for i in range(0, total_token_count, chunk_size):\n",
    "        # chunk_size: i에 더해지는 값  ex) chunk_size = 3 ==> i=0, i=3, i=6...\n",
    "        chunk = total_encoding[i: i+chunk_size]\n",
    "        decoded = encoder.decode(chunk)  # 숫자 list를 다시 text로 바꾼다.\n",
    "        text_list.append(decoded)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list = split_text(full_text, 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. (쪼갠 문서를) 임베딩 -> 벡터 DB에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection: RDB의 Table 같은 개념\n",
    "collection_name = \"tax_collection3\"\n",
    "\n",
    "# 새로운 collection을 만들 때\n",
    "# tax_collection = chroma_client.create_collection(collection_name)\n",
    "\n",
    "# 이미 있는 collection을 가져올 때\n",
    "tax_collection = chroma_client.get_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_embedding = OpenAIEmbeddingFunction(\n",
    "    api_key=openai_api_key,\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_collection = chroma_client.get_or_create_collection(\n",
    "    collection_name,\n",
    "    embedding_function=openai_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "for index in range(len(chunk_list)):\n",
    "    id_list.append(f\"{index}\")\n",
    "    \n",
    "len(chunk_list), len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_collection.add(\n",
    "    documents=chunk_list,\n",
    "    ids=id_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 질문이 있을 때 벡터 DB에서 유사도 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"연봉 5천만 원인 직장인의 소득세는 얼마인가요?\"\n",
    "# embedding 된 텍스트에서 유사도 ㄷ검색 후 답변 return\n",
    "retrieved_doc = tax_collection.query(\n",
    "    query_texts=query,\n",
    "    n_results=3 # n_results: 가져오는 답변의 개수\n",
    ") \n",
    "retrieved_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_doc_list = retrieved_doc[\"documents\"] # retrieved_doc.get(\"documents\")\n",
    "retrieved_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_doc_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 유사도 검색으로 가져온 문서를 LLM에 질문과 같이 전달(LLM 질의)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# \"\"\"\n",
    "#     [사용자] 2020년 월드시리즈 우승자는 누구야?\n",
    "#     [어시스턴트] Los Angeles Dodgers 입니다.\n",
    "#     [사용자] 그 게임은 어디서 플레이됐어?\n",
    "\n",
    "#     - messages를 이용해 위와 같이 얘기한 history를 같이 넘겨주는 것이다.\n",
    "#     - by LLM은 기본적으로 채팅을 지향하기 때문\n",
    "# \"\"\"\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "#         {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "                당신은 한국의 소득세 전문가 입니다. 아래 내용을 참고해서 사용자의 질문에 답변해주세요\n",
    "                {retrieved_doc_list[0]}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content # 원하는 답을 찾아가는 과정이 참 길다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
